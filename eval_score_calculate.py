import os
import json
import tqdm
import argparse
from utils.qwen_api import QwenAPI
from utils.openai_api import OpenaiAPI


prompt = \
"""<<Task>>
{task}

<<Model-generated solution>>
{solution}

<<Judgement reference>>
## Analysis knowledge:
{analysis_knowledge}
## Technology knowledge:
{technology_knowledge}
## Goldn explanation:
{golden_explanation}
## Goldn solution:
{golden_solution}

<<Instruction>>
The above <<Task>> is a complex requirement in an actual engineering scenario. The above <<Model-generated solution>> is a solution generated by a certain model. You are required to evaluate this solution based on the <<Judgement reference>> annotated by a human expert. The <<Judgement reference>> consists of the following components: 
(a) Analysis knowledge: A deep analysis of the various restrictive factors present in this complex requirement. 
(b) Technology knowledge: A detailed explanation of the various technologies that must be used to solve this complex requirement. 
(c) Golden explanation: An explanation of how to use these technologies to overcome various challenges. 
(d) Golden solution: The standard solution provided by human experts.
Your evaluation of the <<Model-generated solution>> must fully consider the <<Judgement reference>>. The specific evaluation requirements are as follows.

<<Requirements>>
1. You need to evaluate the solution above from two dimensions. The range for each of the two scores is an integer between 0 and 100, where the minimum score is 0 and the maximum score is 100.
2. The scoring details for the two dimensions are as follows:
(2.1) Analysis Score: Refer to the aforementioned Analysis knowledge, Golden explanation, and Golden solution to assess whether the <<Model-generated solution>> has thoroughly considered the various restrictive factors in the <<Task>>. Pay special attention to listing each restrictive factor in the Analysis knowledge one by one and evaluating whether the model output has considered these factors. If considered, you need to specify which part of the <<Model-generated solution>> addresses the restrictive factor, and whether this part is sufficiently correct and specific.
(2.1.1) If no factors are considered, score 0.
(2.1.2) If factors are considered but the analysis is not entirely correct, score 11-30 depending on the degree of correctness.
(2.1.3) If factors are considered and the analysis is correct but not specific, score 31-60 depending on the level of specificity.
(2.1.4) If factors are considered, the analysis is correct, and it is specific, score 61-90 based on its similarity to the standard Analysis knowledge.
(2.1.5) If it is fully consistent with the standard Analysis knowledge, score 100.
(2.2) Technology Score: Refer to the aforementioned Technology knowledge, Golden explanation, and Golden solution to evaluate whether the <<Model-generated solution>> has employed appropriate technologies to address the challenges in the <<Task>>. Pay special attention to listing each technology in the Technology knowledge one by one and evaluating whether the model output has used these technologies. If used, you need to specify which part of the <<Model-generated solution>> utilizes the technology, and whether this part is sufficiently correct and specific.
(2.2.1) If no technologies are used, score 0.
(2.2.2) If technologies are used but not entirely correctly, score 11-30 depending on the degree of correctness.
(2.2.3) If technologies are used and correctly applied but not specific, score 31-60 depending on the level of specificity.
(2.2.4) If technologies are used correctly and specifically, score 61-90 based on their similarity to the standard Technology knowledge.
(2.2.5) If it is fully consistent with the standard Technology knowledge, score 100.
3. During the evaluation process, you must first evaluate the solution based on the three dimensions mentioned above and display your reasoning process. After completing your reasoning, you must output the identifier ##Scores## followed by your evaluation results in the form of a dictionary, i.e.: ##Scores## {{"Analysis Score": int, "Technology Score": int}}
4. Note that a longer solution is not necessarily better. The sole basis of your evaluation process is the aforementioned Judgement reference, and your evaluation results must fully take this reference into account.
5. Below is an example you can refer to when completing your evaluation:
## An example of evaluation:
1. For Analysis Score Evaluation. The analysis evaluates whether the solution adequately considers restrictive factors in the task. Let's analyze each factor from the Analysis knowledge and compare it with the model's solution:
For (analysis_0) Uneven soil particle distribution and high groundwater levels: The solution mentions groundwater analysis (in Geotechnical Investigation and Analysis) and dewatering systems (in Water Management). It also discusses preventing water ingress but does not explicitly address piping issues or the specific use of interlocking casing piles to stabilize the foundation pit. While the factors are considered, the analysis lacks detail and specificity regarding solutions such as technology for groundwater cut-off and particle stabilization. Conclusion: Analysis is present and mostly correct but lacks specificity in addressing key risks like piping and groundwater intrusion. Score: 60/100.
For (analysis_1) Deep excavation with moderately weathered limestone: The solution discusses slope stability analysis and phased excavation to control depth, as well as bracing and anchoring systems for stabilization. However, it does not explicitly address challenges such as settlement risks or hard rock excavation in limestone. The application of pre-applied axial force in steel supports is not mentioned, which is critical for ensuring stability. Conclusion: The analysis is partially correct but lacks depth and specificity regarding excavation in moderately weathered limestone and associated risks. Score: 50/100.
For (analysis_2) Sensitive surrounding environment: The solution acknowledges sensitivity in the surrounding environment and mentions real-time monitoring and mitigation measures (e.g., structural impact assessments, emergency response plans). However, it does not provide detailed measures for preventing settlement or damage to underground pipelines or nearby structures, such as dynamic monitoring and quantified safety parameters. Conclusion: The analysis is correct but not specific enough regarding risks to surrounding buildings and underground pipelines. Score: 60/100. 
Overall Analysis Score: While the solution considers the main restrictive factors, it does not address all of them comprehensively or with sufficient specificity. Final Analysis Score: 57/100.
2. For Technology Score Evaluation. The evaluation checks whether appropriate technologies were employed. Let's analyze each technology from the Technology knowledge:
For (technology_0) Casing Interlocking Pile Technology: The solution does not explicitly mention casing interlocking pile technology, which is critical for ensuring foundation pit enclosure and managing groundwater effectively. Instead, it suggests general retaining structures like sheet piles and diaphragm walls, which are less specific for high groundwater conditions. Conclusion: This technology is not applied. Score: 0/100.
For (technology_1) Layered Excavation and Steel Support System: The solution mentions phased excavation and the use of steel supports and bracing but does not include details about pre-applied axial forces, which are vital for controlling deformation and ensuring stability. The specificity of this technology's application is lacking. Conclusion: Partially applied. Score: 50/100.
For (technology_2) Precipitation and Grouting Measures: The solution discusses dewatering systems and drainage channels but omits the use of grouting measures, which are essential for controlling groundwater levels and preventing leakage. Grouting was explicitly required in the reference but is absent here. Conclusion: Partially applied. Score: 40/100.
For (technology_3) Dynamic Monitoring and Protection Measures: The solution includes real-time monitoring with sensors and references emergency response measures. However, it does not quantify safety metrics, such as settlement rates, or specify dynamic adjustment strategies. These omissions limit the specificity of this technology's application. Conclusion: Partially applied. Score: 50/100.
Overall Technology Score: The solution employs some relevant technologies but omits critical ones (e.g., casing interlocking piles and grouting measures) and lacks specificity in others. Final Technology Score: 35/100.
Final Scores
##Scores## {{"Analysis Score": 57, "Technology Score": 35}}

<<Judgement>>"""


def score_helper(data, id_2_knowledge, model_generated_solution, llm):
    analysis_knowledge_list = [
        id_2_knowledge[analysis_id] for analysis_id in data['analysis_ids'] 
    ]
    technology_knowledge_list = [
        id_2_knowledge[technology_id] for technology_id in data['technology_ids'] 
    ]
    
    task = f"{data['requirement']}"
    goldn_solution = data['solution']

    analysis_knowledge = ""
    for knowledge in analysis_knowledge_list:
        strip_str = f"{data['id']}_"
        knowledge_id = knowledge['id'].replace(strip_str, "")
        analysis_knowledge += f"{knowledge_id}: {knowledge['content']}\n"
    analysis_knowledge = analysis_knowledge.strip()

    technology_knowledge = ""
    for knowledge in technology_knowledge_list:
        strip_str = f"{data['id']}_"
        knowledge_id = knowledge['id'].replace(strip_str, "")
        technology_knowledge += f"{knowledge_id}: {knowledge['content']}\n"
    technology_knowledge = technology_knowledge.strip()

    explanation = ""
    for ex in data['explanation']:
        explanation += f"{ex['idx']}: {ex['content']}\n"
    explanation = explanation.strip()

    input_text = prompt.format(
        task=task,
        solution=model_generated_solution,
        analysis_knowledge=analysis_knowledge,
        technology_knowledge=technology_knowledge,
        golden_explanation=explanation,
        golden_solution=goldn_solution
    )

    score = None
    for _ in range(8):
        try: 
            output_text = llm.get_response(input_text)
            if "##Scores##" in output_text:
                raw_score = output_text.split("##Scores##")[-1].strip().strip("#").strip("*").strip(":").strip("```json").strip()
                score = json.loads(raw_score)
                break
            elif "## Scores" in output_text:
                raw_score = output_text.split("## Scores")[-1].strip().strip("#").strip("*").strip(":").strip("```json").strip()
                score = json.loads(raw_score)
                break
            else:
                raise Exception("no score")
        except Exception as e:
            print("output_text", output_text)
            print("error", e)
            print("retry")

    return score, output_text
    

def calculate_score(datas, corpus, generated_result_path, print_info):

    llm = OpenaiAPI()
    model_suffix = ""

    # qwen_url = "10.32.10.224"
    # llm = QwenAPI(
    #     url=f"http://{qwen_url}:1225/v1/chat/completions", 
    # )
    # model_suffix = "_qwen"
    # print(f"=====warning: using QwenAPI api=====")

    id_2_knowledge = {corpu['id']: corpu for corpu in corpus}

    print(f"doing {generated_result_path}")
    generated_results = [json.loads(line) for line in open(generated_result_path)]
    id_2_result = {result['id']: result for result in generated_results}
    assert len(datas) == len(generated_results), f"len(datas) != len(generated_results): {len(datas)} != {len(generated_results)}"

    fw_path = f"{generated_result_path}_score{model_suffix}.jsonl"
    fw = open(fw_path, "a")
    exiting_data_ids = [data['id'] for data in [json.loads(line) for line in open(fw_path)]]
    for i in tqdm.tqdm(range(len(datas)), desc=f"{print_info} {model_suffix}"):
        print(f"\ndoing {i}/{len(datas)}")
        data = datas[i]
        generated_result = id_2_result[data['id']]

        if data['id'] in exiting_data_ids:
            print(f"skip {data['id']}, already_done")
            continue

        model_generated_solution = generated_result['output_text'].replace("#", "").replace("<", "").replace(">", "").strip()
        score, judgement = score_helper(data, id_2_knowledge, model_generated_solution, llm)

        generated_result["judgement"] = judgement
        generated_result["score"] = score

        fw.write(json.dumps(generated_result, ensure_ascii=False) + "\n")
        fw.flush()
    fw.close()

    print(f"done {generated_result_path}")
    return 1


if __name__ == '__main__':

    for scenario in [
        "0_test",
        # "1_environment", 
        # "2_mining",
        # '3_transport',
        # '4_aerospace',
        # '5_telecom',
        # '6_architecture',
        # '7_water',
        # '8_farming',
    ]:

        datas = json.load(open(f"./benchmark/{scenario}/datas.json"))
        corpus = json.load(open(f"./benchmark/{scenario}/corpus.json"))
    
        generated_result_path = f"./eval_results/{scenario}/generated_by_5_ours_5_2_1_.jsonl"
        calculate_score(datas, corpus, generated_result_path, print_info=f"{scenario}")